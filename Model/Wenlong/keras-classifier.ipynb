{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings for KerasWrapper Class\n",
    "keras_settings = {}\n",
    "keras_settings['epochs'] = 20\n",
    "keras_settings['batch_size'] = 100\n",
    "keras_settings['verbose'] = 0\n",
    "keras_settings['validation_split'] = 0.0\n",
    "keras_settings['validation_data'] = None\n",
    "keras_settings['shuffle'] = True\n",
    "\n",
    "keras_settings['optimizer'] = 'adadelta'\n",
    "keras_settings['loss'] = 'binary_crossentropy'\n",
    "keras_settings['metrics'] = ['accuracy']\n",
    "\n",
    "# Wrapper class for Sequential Neural Network\n",
    "class KerasSequential(Sequential):\n",
    "    # settings contains several variables for the fit and score functions\n",
    "    def __init__(self, settings = None, layers=None, name=None):\n",
    "        Sequential.__init__(self, layers, name)\n",
    "        self.settings = settings\n",
    "    \n",
    "    # Fits sequential model to test batch\n",
    "    def fit(self, X, y):\n",
    "        Sequential.compile(self,\n",
    "                           optimizer = self.settings['optimizer'],\n",
    "                           loss = self.settings['loss'],\n",
    "                           metrics = self.settings['metrics'])\n",
    "        return Sequential.fit(self, np.array(X), y,\n",
    "                 epochs = self.settings['epochs'],\n",
    "                 batch_size = self.settings['batch_size'],\n",
    "                 verbose = self.settings['verbose'],\n",
    "                 validation_split = self.settings['validation_split'],\n",
    "                 validation_data = self.settings['validation_data'],\n",
    "                 shuffle = self.settings['shuffle'])\n",
    "    \n",
    "    # Returns the accuracy of the test batch\n",
    "    def score(self, X_test, y_test):\n",
    "        score =  Sequential.evaluate(self, np.array(X_test), y_test,\n",
    "                               batch_size = self.settings['batch_size'],\n",
    "                               verbose = self.settings['verbose'])\n",
    "        return score[1] if self.settings['metrics'] != None else score\n",
    "    \n",
    "    # Reset\n",
    "    def reset(self):\n",
    "        return Sequential.reset_states(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# declare utility functions\n",
    "TARGET_COL = 622\n",
    "FEATURE = 0\n",
    "TARGET = 1\n",
    "\n",
    "def get_csv(path):\n",
    "    data = pd.read_csv(filepath_or_buffer=path, header=None)\n",
    "    # all train data\n",
    "    X = data.iloc[:,4:-1]\n",
    "    # all test data\n",
    "    Y = data.iloc[:, -1:][TARGET_COL]\n",
    "    return (X, Y, data)\n",
    "\n",
    "def part_list(lst, n):\n",
    "    \"\"\"\n",
    "        part_list: Partition lst balanced parts\n",
    "        in: \n",
    "            lst - list that needs to be partitioned\n",
    "            n - integer number of partitions\n",
    "        out:\n",
    "            partitioned list\n",
    "    \"\"\"\n",
    "    parts, rest = divmod(len(lst), n)\n",
    "    lstiter = iter(lst)\n",
    "    for j in xrange(n):\n",
    "        plen = len(lst)/n + (1 if rest > 0 else 0)\n",
    "        rest -= 1\n",
    "        yield list(itertools.islice(lstiter, plen))\n",
    "\n",
    "def build_group_df(data, patients):\n",
    "    \"\"\"\n",
    "        build_group_df: helper for build_cross_validation_sets\n",
    "        in: \n",
    "            data - RAW data\n",
    "            patients - list of patient ids\n",
    "        out:\n",
    "            df with concatenated pixel data relevant to each patient in patients\n",
    "    \"\"\"\n",
    "    return pd.concat([data[data[0] == patient] for patient in patients], ignore_index=True)\n",
    "\n",
    "def build_cross_validation_sets(data, k):\n",
    "    \"\"\"\n",
    "        build_cross_validation_sets: helper for cross_validate\n",
    "        in:\n",
    "            data: RAW data\n",
    "            k - desire number of groups\n",
    "        out:\n",
    "            list of tuples: (feature_df, target_series)\n",
    "    \"\"\"\n",
    "    # manifest constants, get unique patients, and random shuffle\n",
    "    unique_patients = data[0].unique().tolist()\n",
    "    random.shuffle(unique_patients)\n",
    "\n",
    "    #create k groups\n",
    "    k_groups = list(part_list(unique_patients, k))\n",
    "    \n",
    "    # [df1, df2, df3, df4] with each dfi repersenting the ith group in k total groups\n",
    "    k_df = [build_group_df(data, group) for group in k_groups]\n",
    "    # (features, target) for each df\n",
    "    k_df_split = [(data.iloc[:,4:-1], data.iloc[:, -1:][TARGET_COL]) for data in k_df]\n",
    "    \n",
    "    return k_df_split\n",
    "    \n",
    "def cross_validate(model, data, k = 5):\n",
    "    \"\"\"\n",
    "        cross_validate: performs cross validation\n",
    "        in:\n",
    "            model - input model\n",
    "            data - RAW data\n",
    "            k - desired number of groups\n",
    "        out:\n",
    "            (mean of scores, list of scores)\n",
    "    \"\"\"\n",
    "    # manifest constants\n",
    "    score_list = []\n",
    "    \n",
    "    # get split data\n",
    "    k_df_split = build_cross_validation_sets(data, k)\n",
    "    \n",
    "    for (i, (X, y)) in enumerate(k_df_split):\n",
    "        # get all dfs not k\n",
    "        non_kth_group = k_df_split[:]\n",
    "        del non_kth_group[i]\n",
    "        \n",
    "        # build x and y train data\n",
    "        X_train = pd.concat([data[FEATURE] for data in non_kth_group])\n",
    "        y_train = pd.concat([data[TARGET] for data in non_kth_group])\n",
    "        \n",
    "        # build x and y test data\n",
    "        X_test = X\n",
    "        y_test = y\n",
    "        \n",
    "        # train model on non_kth_group\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # test model on kth group\n",
    "        score = model.score(X_test, y_test)\n",
    "        \n",
    "        # add score to score list\n",
    "        score_list.append(score)\n",
    "        \n",
    "    return (np.mean(score_list), score_list)\n",
    "\n",
    "#data.sort_values(0)\n",
    "#build_cross_validation_sets(data, 5)\n",
    "#cross_validate(LRmodel, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X, y, data = get_csv('train_data.csv')\n",
    "\n",
    "# Split data into random 80/20 train / test split\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "cut = int(len(X)*0.8)\n",
    "X_train = X.iloc[indices[:cut],:]\n",
    "y_train = y[indices[:cut]]\n",
    "X_test = X.iloc[indices[cut:],:]\n",
    "y_test = y[indices[cut:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different models to test\n",
    "'''\n",
    "model1 - 2x Dense\n",
    "model2 - 2x Dense, Dense Dropout\n",
    "model3 - 2x Dense, Dense Dropout, Input Dropout\n",
    "model4 - 4x Dense\n",
    "model5 - 4x Dense, Dense Dropout\n",
    "model5 - 4x Dense, Dense Dropout, Input Dropout\n",
    "'''\n",
    "models = {}\n",
    "models['model1'] = KerasSequential(keras_settings,\n",
    "                       [Dense(200, activation='relu', input_dim=618),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])\n",
    "models['model2'] = KerasSequential(keras_settings,\n",
    "                       [Dense(200, activation='relu', input_dim=618),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])\n",
    "models['model3'] = model = KerasSequential(keras_settings,\n",
    "                       [Dropout(0.2, input_shape=(618,)),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])\n",
    "models['model4'] = KerasSequential(keras_settings,\n",
    "                       [Dense(200, activation='relu', input_dim=618),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])\n",
    "models['model5'] = KerasSequential(keras_settings,\n",
    "                       [Dense(200, activation='relu', input_dim=618),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])\n",
    "models['model6'] = KerasSequential(keras_settings,\n",
    "                       [Dropout(0.2, input_shape=(618,)),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(200, activation='relu'),\n",
    "                        Dropout(0.5),\n",
    "                        Dense(1, activation='sigmoid')\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model3\n",
      "\tCross validation accuracy:\t0.8239\n",
      "\tRandom split accuracy:\t\t0.8585\n",
      "model2\n",
      "\tCross validation accuracy:\t0.7924\n",
      "\tRandom split accuracy:\t\t0.9010\n",
      "model1\n",
      "\tCross validation accuracy:\t0.8299\n",
      "\tRandom split accuracy:\t\t0.9530\n",
      "model6\n"
     ]
    }
   ],
   "source": [
    "# Test all variations of neural network model\n",
    "for model_name in models:\n",
    "    print(model_name)\n",
    "    m = models[model_name]\n",
    "    \n",
    "    # Cross validation accuracy\n",
    "    print(\"\\tCross validation accuracy:\\t{:0.4f}\".format(cross_validate(m, data, 5)[0]))\n",
    "    \n",
    "    m.reset()\n",
    "    \n",
    "    # 80/20 split accuracy\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_test, y_test)\n",
    "    print(\"\\tRandom split accuracy:\\t\\t{:0.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best variation of the network is the 4th model (4x dense layer, no dropout)\n",
    "# Save neural network layer weights\n",
    "m = models['model4']\n",
    "m.reset()\n",
    "cross_validate(m, data, 5)[0]\n",
    "m.save('keras_classifier.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

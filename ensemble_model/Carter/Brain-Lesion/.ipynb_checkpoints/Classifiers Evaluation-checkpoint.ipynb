{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import random\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75556997526008052,\n",
       " [0.74952865761689291,\n",
       "  0.7935023134178234,\n",
       "  0.78410898965791564,\n",
       "  0.6877054569362262,\n",
       "  0.76300445867154454])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# declare utility functions\n",
    "TARGET_COL = 622\n",
    "FEATURE = 0\n",
    "TARGET = 1\n",
    "\n",
    "def get_csv(path):\n",
    "    data = pd.read_csv(filepath_or_buffer=path, header=None)\n",
    "    # all train data\n",
    "    X = data.iloc[:,4:-1]\n",
    "    # all test data\n",
    "    Y = data.iloc[:, -1:][TARGET_COL]\n",
    "    return (X, Y, data)\n",
    "\n",
    "def part_list(lst, n):\n",
    "    \"\"\"\n",
    "        part_list: Partition lst balanced parts\n",
    "        in: \n",
    "            lst - list that needs to be partitioned\n",
    "            n - integer number of partitions\n",
    "        out:\n",
    "            partitioned list\n",
    "    \"\"\"\n",
    "    parts, rest = divmod(len(lst), n)\n",
    "    lstiter = iter(lst)\n",
    "    for j in xrange(n):\n",
    "        plen = len(lst)/n + (1 if rest > 0 else 0)\n",
    "        rest -= 1\n",
    "        yield list(itertools.islice(lstiter, plen))\n",
    "\n",
    "def build_group_df(data, patients):\n",
    "    \"\"\"\n",
    "        build_group_df: helper for build_cross_validation_sets\n",
    "        in: \n",
    "            data - RAW data\n",
    "            patients - list of patient ids\n",
    "        out:\n",
    "            df with concatenated pixel data relevant to each patient in patients\n",
    "    \"\"\"\n",
    "    return pd.concat([data[data[0] == patient] for patient in patients], ignore_index=True)\n",
    "\n",
    "def build_cross_validation_sets(data, k):\n",
    "    \"\"\"\n",
    "        build_cross_validation_sets: helper for cross_validate\n",
    "        in:\n",
    "            data: RAW data\n",
    "            k - desire number of groups\n",
    "        out:\n",
    "            list of tuples: (feature_df, target_series)\n",
    "    \"\"\"\n",
    "    # manifest constants, get unique patients, and random shuffle\n",
    "    unique_patients = data[0].unique().tolist()\n",
    "    random.shuffle(unique_patients)\n",
    "\n",
    "    #create k groups\n",
    "    k_groups = list(part_list(unique_patients, k))\n",
    "    \n",
    "    # [df1, df2, df3, df4] with each dfi repersenting the ith group in k total groups\n",
    "    k_df = [build_group_df(data, group) for group in k_groups]\n",
    "    # (features, target) for each df\n",
    "    k_df_split = [(data.iloc[:,3:-1], data.iloc[:, -1:][TARGET_COL]) for data in k_df]\n",
    "    \n",
    "    return k_df_split\n",
    "    \n",
    "def cross_validate(model, data, k = 5):\n",
    "    \"\"\"\n",
    "        cross_validate: performs cross validation\n",
    "        in:\n",
    "            model - input model\n",
    "            data - RAW data\n",
    "            k - desired number of groups\n",
    "        out:\n",
    "            (mean of scores, list of scores)\n",
    "    \"\"\"\n",
    "    # manifest constants\n",
    "    score_list = []\n",
    "    \n",
    "    # get split data\n",
    "    k_df_split = build_cross_validation_sets(data, k)\n",
    "    \n",
    "    for (i, (X, y)) in enumerate(k_df_split):\n",
    "        # get all dfs not k\n",
    "        non_kth_group = k_df_split[:]\n",
    "        del non_kth_group[i]\n",
    "        \n",
    "        # build x and y train data\n",
    "        X_train = pd.concat([data[FEATURE] for data in non_kth_group])\n",
    "        y_train = pd.concat([data[TARGET] for data in non_kth_group])\n",
    "        \n",
    "        # build x and y test data\n",
    "        X_test = X\n",
    "        y_test = y\n",
    "        \n",
    "        # train model on non_kth_group\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # test model on kth group\n",
    "        score = model.score(X_test, y_test)\n",
    "        \n",
    "        # add score to score list\n",
    "        score_list.append(score)\n",
    "        \n",
    "    return (np.mean(score_list), score_list)\n",
    "\n",
    "#data.sort_values(0)\n",
    "#build_cross_validation_sets(data, 5)\n",
    "cross_validate(LRmodel, data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>613</th>\n",
       "      <th>614</th>\n",
       "      <th>615</th>\n",
       "      <th>616</th>\n",
       "      <th>617</th>\n",
       "      <th>618</th>\n",
       "      <th>619</th>\n",
       "      <th>620</th>\n",
       "      <th>621</th>\n",
       "      <th>622</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>528</td>\n",
       "      <td>158</td>\n",
       "      <td>112</td>\n",
       "      <td>23</td>\n",
       "      <td>0.150380</td>\n",
       "      <td>0.146330</td>\n",
       "      <td>0.142520</td>\n",
       "      <td>0.153820</td>\n",
       "      <td>0.148660</td>\n",
       "      <td>0.138230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149050</td>\n",
       "      <td>0.148270</td>\n",
       "      <td>0.140620</td>\n",
       "      <td>0.133380</td>\n",
       "      <td>0.134310</td>\n",
       "      <td>0.138130</td>\n",
       "      <td>0.139070</td>\n",
       "      <td>0.134450</td>\n",
       "      <td>0.124640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367</td>\n",
       "      <td>89</td>\n",
       "      <td>173</td>\n",
       "      <td>20</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>0.194210</td>\n",
       "      <td>0.215200</td>\n",
       "      <td>0.204170</td>\n",
       "      <td>0.202180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123590</td>\n",
       "      <td>0.120340</td>\n",
       "      <td>0.116290</td>\n",
       "      <td>0.114440</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>0.110490</td>\n",
       "      <td>0.108930</td>\n",
       "      <td>0.106310</td>\n",
       "      <td>0.104780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>283</td>\n",
       "      <td>190</td>\n",
       "      <td>55</td>\n",
       "      <td>17</td>\n",
       "      <td>0.027484</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.019973</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114170</td>\n",
       "      <td>0.111660</td>\n",
       "      <td>0.111410</td>\n",
       "      <td>0.111810</td>\n",
       "      <td>0.110960</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.109130</td>\n",
       "      <td>0.108310</td>\n",
       "      <td>0.106670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>424</td>\n",
       "      <td>166</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>0.174210</td>\n",
       "      <td>0.170880</td>\n",
       "      <td>0.174460</td>\n",
       "      <td>0.190960</td>\n",
       "      <td>0.182380</td>\n",
       "      <td>0.176380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109080</td>\n",
       "      <td>0.103190</td>\n",
       "      <td>0.096503</td>\n",
       "      <td>0.091461</td>\n",
       "      <td>0.088297</td>\n",
       "      <td>0.085691</td>\n",
       "      <td>0.084188</td>\n",
       "      <td>0.082517</td>\n",
       "      <td>0.081374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>221</td>\n",
       "      <td>249</td>\n",
       "      <td>25</td>\n",
       "      <td>0.055317</td>\n",
       "      <td>0.058417</td>\n",
       "      <td>0.059609</td>\n",
       "      <td>0.042918</td>\n",
       "      <td>0.049833</td>\n",
       "      <td>0.056509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.021334</td>\n",
       "      <td>0.021952</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.023188</td>\n",
       "      <td>0.023806</td>\n",
       "      <td>0.024424</td>\n",
       "      <td>0.025042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 623 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3         4         5         6         7         8    \\\n",
       "0  528  158  112   23  0.150380  0.146330  0.142520  0.153820  0.148660   \n",
       "1  367   89  173   20  0.212500  0.197260  0.194210  0.215200  0.204170   \n",
       "2  283  190   55   17  0.027484  0.036190  0.046603  0.019973  0.032946   \n",
       "3  424  166   90   29  0.174210  0.170880  0.174460  0.190960  0.182380   \n",
       "4   18  221  249   25  0.055317  0.058417  0.059609  0.042918  0.049833   \n",
       "\n",
       "        9   ...        613       614       615       616       617       618  \\\n",
       "0  0.138230 ...   0.149050  0.148270  0.140620  0.133380  0.134310  0.138130   \n",
       "1  0.202180 ...   0.123590  0.120340  0.116290  0.114440  0.111110  0.110490   \n",
       "2  0.047286 ...   0.114170  0.111660  0.111410  0.111810  0.110960  0.109900   \n",
       "3  0.176380 ...   0.109080  0.103190  0.096503  0.091461  0.088297  0.085691   \n",
       "4  0.056509 ...   0.020098  0.020716  0.021334  0.021952  0.022570  0.023188   \n",
       "\n",
       "        619       620       621  622  \n",
       "0  0.139070  0.134450  0.124640    1  \n",
       "1  0.108930  0.106310  0.104780    1  \n",
       "2  0.109130  0.108310  0.106670    0  \n",
       "3  0.084188  0.082517  0.081374    1  \n",
       "4  0.023806  0.024424  0.025042    1  \n",
       "\n",
       "[5 rows x 623 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use path to get data\n",
    "path = \"/home/carter/Documents/Brain-Lesion-Predictive-Model/Data/MRI-DATA/train_data.csv\"\n",
    "(X, Y, data) = get_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model creation\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82704"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = cross_val_score(LRmodel, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82015982864637249"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model creation\n",
    "MLPmodel = MLPClassifier()\n",
    "MLPmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90688000000000002"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87937718495537198"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_score = cross_val_score(MLPmodel, X, Y, cv=5)\n",
    "MLP_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
